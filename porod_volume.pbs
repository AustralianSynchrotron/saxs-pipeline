#!/bin/bash
#PBS -V
#PBS -N job-porod-volume
#PBS -A ASync011
#PBS -M joanna.huang@synchrotron.org.au -m abe
#PBS -l nodes=1:ppn=1
#PBS -l walltime=00:20:00

cd $PBS_O_WORKDIR

#--------- porod_volume.pbs ---------------------------------------------------#
# This is a PBS job to run the following models in order on SAXS experimental 
# data file specified,
#
#     1. AUTORG
#     2. DATGNOM
#     3. DATPOROD
#
# Input file: 
# 
#     1. The SAXS experimental data files (*.dat) in ASCII format.
#
# Output file: 
#
#     1. GNOM output file (*.out)
#
# To run porod_volume.pbs using qsub:
#
#     qsub -v dat_file=DAT_FILE,output_path=OUTPUT_PATH,pipeline_home=PIPELINE_HOME,prod_ssh_access=SSH_ACCESS,prod_scp_dest=SCP_DEST,prod_pipeline_harvest=HARVEST porod_volume.pbs
#
# where 
#
#     DAT_FILE      A full path of your SAS experimental data file to be used 
#                   for models.
#     OUTPUT_PATH   A full directory path for all output files generated 
#                   during pipeline modelling. 
#     PIPELINE_HOME A full absolute path of home directory of Pipeline source 
#                   code on MASSIVE.
#     SSH_ACCESS    A string of ssh username and remote hostname used to connect
#                   to SAXS production server.
#     SCP_DEST      It is a remote full directory path for this pipeline script
#                   to copy output files back to remote SAXS production server.
#     HARVEST       It is a remote full path to trigger pipeline harvest script
#                   on remote SAXS production server.

python "$pipeline_home"/porod_volume.py -d $dat_file -o $output_path -s $prod_ssh_access -d $prod_scp_dest -h $prod_pipeline_harvest